{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "from solution import *\n",
    "import datetime as dt\n",
    "\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "from matplotlib import dates as mdates\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tПолучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers = ['MGNT', 'DIXY']\n",
    "quote_d = dict()\n",
    "msfo_d = dict()\n",
    "\n",
    "for ticker in tickers:\n",
    "    quote_d[ticker] = pd.read_csv('..\\\\3. Data preparation\\\\QUOTE_'+ ticker + '.csv', sep=';', parse_dates=[0], dayfirst=True, encoding='cp1251')\n",
    "    quote_d[ticker].columns = ['date', 'close_pur', 'close_sell', 'open', 'low', 'high', 'close', 'avg' ,'vol', 'deal']\n",
    "    \n",
    "    msfo_d[ticker] = pd.read_csv('..\\\\3. Data preparation\\\\MSFO_NA_'+ ticker + '.csv', sep=';', parse_dates=[1], dayfirst=True, encoding = 'cp1251')\n",
    "    # Не все данные нужны на начальном этапе\n",
    "    #msfo_d[ticker] = msfo_d[ticker][['date', 'rep_period', 'revenue', 'revenue_usd', 'revenue_rel', 'revenue_usd_rel', 'net_profit', 'net_profit_rel', 'net_profit_margin', 'inflation']] #\n",
    "    #sfo_dates = msfo_df['date']\n",
    "    \n",
    "#rsbu_df = pd.read_csv('..\\\\3. Data preparation\\\\MGNT_RSBU.csv', sep=';', parse_dates=[1], encoding = 'cp1251')\n",
    "#sfo_df = pd.read_csv('..\\\\3. Data preparation\\\\MSFO_NA_MGNT.csv', sep=';', parse_dates=[1], dayfirst=True, encoding = 'cp1251')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "quote_d['DIXY'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quote_d['DIXY'].info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "msfo_d['MGNT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tПредобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ticker in tickers:\n",
    "    # Обработка котировок\n",
    "    quote_d[ticker] = quote_d[ticker].sort_values(['date']).reset_index(drop=True)\n",
    "    quote_d[ticker] = quote_d[ticker][quote_d[ticker]['deal'] != 0]\n",
    "    quote_d[ticker]['open_ln_rel'] = LnRelTransformer(['open']).transform(quote_d[ticker])\n",
    "\n",
    "    # Получаем целевой показатель: Цена открытия на завтрашний день, относительно сегодняшней\n",
    "    quote_d[ticker]['open_ln_rel_shift'] = ShiftTransformer(['open_ln_rel'], 1).transform(quote_d[ticker])\n",
    "    quote_d[ticker]['vol_ln'] = LnTransformer(['vol']).transform(quote_d[ticker])\n",
    "\n",
    "    # Обработка МСФО\n",
    "    msfo_d[ticker]['rep_period_month'] = msfo_d[ticker].apply(lambda x: int(x['rep_period'].split(\"мес. \",1)[0]), axis=1)\n",
    "    msfo_d[ticker]['rep_period_year'] = msfo_d[ticker].apply(lambda x: int(x['rep_period'].split(\"мес. \",1)[1]), axis=1)\n",
    "    msfo_d[ticker] = msfo_d[ticker].sort_values(['rep_period_year', 'rep_period_month']).reset_index(drop=True)\n",
    "\n",
    "    # Пересчёт на 3 месяца\n",
    "    columns = ['revenue', 'revenue_usd', 'net_profit']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column+'_q'] = msfo_d[ticker].apply(lambda x: (x[column] - msfo_d[ticker][column][x.name-1]) if ((x.name > 0) & (x['rep_period_month'] != 3)) else (x[column]), axis=1)\n",
    "\n",
    "    # Пересчёт на 3 месяца инфляции\n",
    "    columns = ['inflation']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column+'_q'] = msfo_d[ticker].apply(lambda x: ((1 + x[column]) / (1 + msfo_d[ticker][column][x.name-1])) if ((x.name > 0) & (x['rep_period_month'] != 3)) else (1 + x[column]), axis=1)\n",
    "\n",
    "    # Пересчёт на год инфляции\n",
    "    column = 'inflation_q'\n",
    "    msfo_d[ticker]['inflation_year'] = msfo_d[ticker].apply(lambda x: (x[column] * msfo_d[ticker][column][x.name-1] * msfo_d[ticker][column][x.name-2] * msfo_d[ticker][column][x.name-3]) if (x.name > 3) else (None), axis=1)\n",
    "\n",
    "    # Доля от кварталной выручки\n",
    "    columns = ['net_profit_q']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column+'_margin'] = msfo_d[ticker][column] / msfo_d[ticker]['revenue_q']\n",
    "\n",
    "    # Изменение годовое\n",
    "    columns = ['net_profit_margin', 'net_profit_q_margin']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column+'_dif'] = msfo_d[ticker].apply(lambda x: (x[column] - msfo_d[ticker][column][x.name-4]) if (x.name > 3) else (None), axis=1)\n",
    "\n",
    "    #Логарифм\n",
    "    columns = ['inflation_q', 'inflation_year']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column+'_ln'] = np.log(msfo_d[ticker][column])\n",
    "\n",
    "    #Логарифм от > 1\n",
    "    #columns_ln = ['revenue_q', 'revenue_usd_q']\n",
    "    #for column in columns_ln:\n",
    "    #    msfo_d[ticker][column+'_ln'] = msfo_d[ticker].apply(lambda x: np.log(x[column]) if x[column] > 1 else 0, axis=1)    \n",
    "\n",
    "    # Логарифм относительного роста по сравнению с показателями годичной давности\n",
    "    # Вернуться к показателям сравнения годовалого\n",
    "    columns = ['revenue', 'revenue_q', 'revenue_usd', 'revenue_usd_q']\n",
    "    for column in columns:\n",
    "        #rsbu_df[column + '_ln_rel'] = LnRelTransformer([column]).transform(rsbu_df)\n",
    "        msfo_d[ticker][column + '_ln_rel_year'] = LnRelYearTransformer([column]).transform(msfo_d[ticker])    \n",
    "\n",
    "    # Логарифм относительного роста с учётом инфляции\n",
    "    columns = ['revenue_ln_rel_year', 'revenue_q_ln_rel_year']\n",
    "    for column in columns:\n",
    "        msfo_d[ticker][column + '_inf'] = msfo_d[ticker][column] - msfo_d[ticker]['inflation_year_ln']\n",
    "\n",
    "\n",
    "    msfo_d[ticker] = msfo_d[ticker].drop([\n",
    "                           #'rep_period',\n",
    "                           'rep_period_year',\n",
    "                           #'rep_period_month',\n",
    "                           'revenue',\n",
    "                           'revenue_q',\n",
    "                           'revenue_usd', \n",
    "                           'revenue_usd_q',\n",
    "                           'net_profit',\n",
    "                           'net_profit_q',\n",
    "                           'inflation',\n",
    "                           'inflation_q',\n",
    "                           'inflation_year',\n",
    "                           'inflation_q_ln'\n",
    "                           ], axis = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "row_df = pd.DataFrame (columns = msfo_df.columns)\n",
    "row_s = pd.Series({'date': pd.tslib.Timestamp(0), \n",
    "                   'rep_period_year': 0, #!!!!!!\n",
    "                   #'revenue_rel' :0,\n",
    "                   #'revenue_usd_rel' :0,\n",
    "                   #'revenue_q_ln': 0,\n",
    "                   'revenue_ln_rel_year': 0,\n",
    "                   'revenue_q_ln_rel_year': 0, \n",
    "                   'revenue_usd_ln_rel_year': 0,\n",
    "                   'revenue_usd_q_ln_rel_year': 0\n",
    "                  })\n",
    "\n",
    "# Дополнение отчётных данных msfo нулевыми строками в дни, ктогда велись торги\n",
    "#msfo_df = pd.merge (init_df[['date']], msfo_df, how='left', on=['date'])\n",
    "#for index in msfo_df.index:\n",
    "#    if not (msfo_df.loc[index]['date'] in list(msfo_dates)):\n",
    "#        row_s['date'] = msfo_df.loc[index]['date'].date()\n",
    "#        msfo_df.loc[index] = row_s"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "msfo_df[msfo_df['date'].isin(msfo_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Объединение всех данных в одну строку с присвоением тикера\n",
    "init_d = dict()\n",
    "for ticker in tickers:\n",
    "    init_d[ticker] = pd.merge(quote_d[ticker], msfo_d[ticker], how='left', on=['date'])\n",
    "    # Делаем one_hot для тикеров\n",
    "    for ticker2 in tickers:\n",
    "        if ticker == ticker2:\n",
    "            init_d[ticker]['ticker_' + ticker2] = 1\n",
    "        else:\n",
    "            init_d[ticker]['ticker_' + ticker2] = 0\n",
    "            \n",
    "    # Внимательно отрезать данные, просматривая относительные показатели\n",
    "    init_d[ticker] = init_d[ticker][init_d[ticker]['date'] > '2008-04-01']\n",
    "\n",
    "    # Оставляем только дни, в которые были опубликованы результаты MSFO\n",
    "    init_d[ticker] = init_d[ticker][init_d[ticker]['date'].isin(msfo_d[ticker]['date'])].reset_index(drop = True)\n",
    "    #del init_df['date']\n",
    "\n",
    "    # Преобразовываем формат из float в int (он сбился, т.к. были значения NA)\n",
    "    init_d[ticker]['rep_period_month'] = init_d[ticker]['rep_period_month'].astype(int)  \n",
    "    \n",
    "    writer = pd.ExcelWriter('..\\\\3. Data preparation\\\\Temp\\\\init_'+ ticker + '.xlsx')\n",
    "    init_d[ticker].to_excel(writer, 'Информация', index = False)\n",
    "    writer.save()\n",
    "\n",
    "# Проверка, что размеры всех данных для каждого тикера равны\n",
    "init_ticker_shape = init_d[tickers[0]].shape\n",
    "state_ok = True\n",
    "\n",
    "for ticker in tickers:\n",
    "    if (init_d[ticker].shape != init_ticker_shape):\n",
    "        state_ok = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Вариант классификатора\n",
    "def set_target_class(init_df, data_cols):\n",
    "    target_buy = init_df[data_cols].apply(lambda x: 1 if x.item() > 0 else 0, axis=1)\n",
    "    target_sell = init_df[data_cols].apply(lambda x: 1 if x.item() <= 0 else 0, axis=1)\n",
    "    return target_buy, target_sell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Объединение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_df = pd.DataFrame (columns = init_d[tickers[0]].columns)\n",
    "\n",
    "for ticker in tickers:\n",
    "    init_df = init_df.append(init_d[ticker], ignore_index=True)\n",
    "    \n",
    "    # Преобразовываем формат из float в int (он сбился, т.к. были значения NA)\n",
    "    init_df['rep_period_month'] = init_df['rep_period_month'].astype(int)  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for ticker in tickers:\n",
    "    #Вариант классификатора\n",
    "    #init_d[ticker]['Target_buy'], init_d[ticker]['Target_sell'] = set_target_class(init_d[ticker], ['OPEN'])\n",
    "\n",
    "    init_d[ticker] = init_d[ticker][list(set(index_cols + data_cols + target_cols))]\n",
    "\n",
    "    # Внимательно отрезать данные, просматривая относительные показатели\n",
    "    init_d[ticker] = init_d[ticker][init_d[ticker]['date'] > '2008-04-01']\n",
    "\n",
    "    # Оставляем только дни, в которые были опубликованы результаты MSFO\n",
    "    init_d[ticker] = init_d[ticker][init_d[ticker]['date'].isin(msfo_d[ticker]['date'])].reset_index(drop = True)\n",
    "    #del init_df['date']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "init_d['MGNT'].corr('pearson')[['open_ln_rel_shift']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "init_d['DIXY'].corr('pearson')[['open_ln_rel_shift']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "sns.pairplot(init_d['MGNT'][['open_ln_rel_shift', 'net_profit_margin_dif']], palette=\"husl\") #, hue=\"rep_period_year\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sns.pairplot(init_d['DIXY'][['open_ln_rel_shift', 'net_profit_margin_dif']], palette=\"husl\") #, hue=\"rep_period_year\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B> Применение общих преобразований"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_onehot(dataframe, columns):\n",
    "    for column in (columns):\n",
    "        if (dataframe[column].dtype != object):\n",
    "            dataframe[column] = (dataframe[column]).astype('object')\n",
    "        dataframe_encoded = pd.get_dummies(dataframe[column], prefix = column)\n",
    "        dataframe = dataframe.join(dataframe_encoded)\n",
    "    dataframe = dataframe.drop(columns,axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_df = encode_onehot(init_df, ['rep_period_month'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer = pd.ExcelWriter('..\\\\3. Data preparation\\\\Temp\\\\init_df.xlsx')\n",
    "init_df.to_excel(writer, 'Информация', index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Разделение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tt_split(init_df, column = None, value = None, part = None, train_count = None):\n",
    "    '''Split initial dataframe to train, test dataframe by values\n",
    "    part [0;1]'''\n",
    "    if not(part is None):\n",
    "        init_size = init_df.shape[0]\n",
    "        train_size = int(init_size * part)\n",
    "        test_size = init_size - train_size\n",
    "        train_df = init_df[:train_size]\n",
    "        test_df = init_df[train_size:]\n",
    "    elif not(value is None):\n",
    "        train_df = init_df[init_df[column] < value]\n",
    "        test_df = init_df[init_df[column] >= value]\n",
    "    else:\n",
    "        train_df = init_df[:train_count]\n",
    "        test_df = init_df[train_count:]\n",
    "    return train_df , test_df\n",
    "\n",
    "def idt_split(init_df, index_cols, data_cols, target_cols):\n",
    "    index_df = init_df[index_cols]\n",
    "    data_df = init_df[data_cols]\n",
    "    target_df =  init_df[target_cols]\n",
    "    return index_df, data_df, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_cols = ['date']\n",
    "data_cols = ['open_ln_rel',\n",
    "             #'rep_period_year',\n",
    "             #'rep_period_month',\n",
    "             #'inflation_q',\n",
    "             #'inflation_year',\n",
    "             #'inflation_q_ln',\n",
    "             'inflation_year_ln',\n",
    "             'revenue_rel',\n",
    "             #'revenue_usd_rel',\n",
    "             'revenue_ln_rel_year',\n",
    "             'revenue_q_ln_rel_year',\n",
    "             'revenue_usd_ln_rel_year',\n",
    "             'revenue_usd_q_ln_rel_year',\n",
    "             'revenue_ln_rel_year_inf',\n",
    "             'revenue_q_ln_rel_year_inf',\n",
    "             'revenue_rel', \n",
    "             'revenue_usd_rel',\n",
    "             'net_profit_rel',\n",
    "             'net_profit_margin',\n",
    "             'net_profit_q_margin',\n",
    "             'net_profit_margin_dif',\n",
    "             'net_profit_q_margin_dif']\n",
    "for ticker in tickers:\n",
    "    data_cols.append('ticker_' + ticker)\n",
    "\n",
    "for column in list(init_df.columns):\n",
    "    if (column[:len('rep_period_month_')] == 'rep_period_month_'):\n",
    "        data_cols.append(column)\n",
    "\n",
    "#Версия для регрессии\n",
    "target_cols = ['open_ln_rel_shift']\n",
    "\n",
    "#Версия для классификации\n",
    "#target_cols = ['Target_buy', 'Target_sell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_d = dict()\n",
    "init_train_d = dict()\n",
    "init_test_d = dict()\n",
    "for ticker in tickers:\n",
    "    init_d[ticker] = init_df[init_df['ticker_' + ticker] == 1]\n",
    "    \n",
    "    init_train_d[ticker], init_test_d[ticker] = tt_split(init_d[ticker], part = 1) #0.8\n",
    "    #init_train_d[ticker], init_test_d[ticker] = tt_split(init_d[ticker], train_count = init_d[ticker].shape[0]-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_train_d = dict()\n",
    "data_train_d = dict()\n",
    "target_train_d = dict()\n",
    "index_test_d = dict()\n",
    "data_test_d = dict()\n",
    "target_test_d = dict()\n",
    "\n",
    "for ticker in tickers:\n",
    "    index_train_d[ticker], data_train_d[ticker], target_train_d[ticker] = idt_split(init_train_d[ticker], index_cols, data_cols, target_cols)\n",
    "    index_test_d[ticker], data_test_d[ticker], target_test_d[ticker] = idt_split(init_test_d[ticker], index_cols, data_cols, target_cols)\n",
    "    \n",
    "# Объединение показателей тикеров\n",
    "data_train = pd.DataFrame (columns = data_train_d[tickers[0]].columns)\n",
    "data_test = pd.DataFrame (columns = data_test_d[tickers[0]].columns)\n",
    "target_train = pd.DataFrame (columns = target_train_d[tickers[0]].columns)\n",
    "target_test = pd.DataFrame (columns = target_test_d[tickers[0]].columns)\n",
    "\n",
    "for ticker in tickers:\n",
    "    data_train = data_train.append(data_train_d[ticker], ignore_index=True)\n",
    "    data_test = data_test.append(data_test_d[ticker], ignore_index=True)\n",
    "    target_train = target_train.append(target_train_d[ticker], ignore_index=True)\n",
    "    target_test = target_test.append(target_test_d[ticker], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#!!!\n",
    "index_df = index_df[index_df['date'].isin(msfo_dates)]\n",
    "data_df = data_df[data_df.index.isin(index_df.index)]\n",
    "target_df = target_df[target_df.index.isin(index_df.index)]\n",
    "\n",
    "index_df.reset_index(inplace = True, drop = True)\n",
    "data_df.reset_index(inplace = True, drop = True)\n",
    "target_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "#!!!\n",
    "#for ind in data_df.index:\n",
    "#    data_df.loc[ind]['OPEN'] = ind\n",
    "#    target_df.loc[ind]['OPEN'] = ind*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define base mode\n",
    "max_look_back = 1\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = ['dnn', 'lstm', 'lstm_mbb']\n",
    "\n",
    "solutions = dict()\n",
    "DNN_S = getattr(importlib.import_module('dnn_s'), 'DNN_S')\n",
    "solutions['dnn'] = DNN_S(features = data_train.shape[1],\n",
    "                         look_back = max_look_back,\n",
    "                         batch_size = 7, \n",
    "                         nb_epoch=10, \n",
    "                         verbose = 0)\n",
    "\n",
    "#LBTransformer = getattr(importlib.import_module('dnn_s'), 'LBTransformer')\n",
    "\n",
    "LSTM_S = getattr(importlib.import_module('lstm_s'), 'LSTM_S')\n",
    "solutions['lstm'] = LSTM_S(features = data_train.shape[1],\n",
    "                           look_back = max_look_back,\n",
    "                           batch_size = 7, \n",
    "                           nb_epoch=10, \n",
    "                           verbose = 0)\n",
    "\n",
    "LSTM_MBB_S = getattr(importlib.import_module('lstm_mbb_s'), 'LSTM_MBB_S')\n",
    "solutions['lstm_mbb'] = LSTM_MBB_S(features = data_train.shape[1],\n",
    "                                   look_back = max_look_back,\n",
    "                                   batch_size = 7, \n",
    "                                   nb_epoch=32,\n",
    "                                   verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Подбор параметров по сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_grid (grid_cv, data_train, target_train, print_score = False, print_params=False):\n",
    "    grid_cv.fit(data_train, target_train) #, n_jobs=-1\n",
    "    best_estimator_ = grid_cv.best_estimator_\n",
    "    best_score_ = grid_cv.best_score_\n",
    "    if (print_score):\n",
    "        print('Best score: ', best_score_)\n",
    "    if (print_params):\n",
    "        print('Best parameters:')\n",
    "        for param in grid_cv.best_params_:\n",
    "            print(param, ': ', grid_cv.best_params_[param])\n",
    "    return best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def scorer_(estimator, X, y):\n",
    "    # Your criterion here\n",
    "    if np.allclose(estimator.coef_, np.zeros_like(estimator.coef_)):\n",
    "        return 0\n",
    "    else:\n",
    "        return estimator.score(X, y)\n",
    "\n",
    "learner = sklearn.grid_search.GridSearchCV(...\n",
    "                                           scoring=scorer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_text = '_activation_optimizer_b_W_regularizer'\n",
    "\n",
    "# Кастомное разбиение на папки для LSTM_MBB\n",
    "train_indices=[]\n",
    "test_indices=[]\n",
    "indices = np.arange(data_train.shape[0])\n",
    "current = 0\n",
    "for ticker in tickers:\n",
    "    train_indices.extend(np.arange(current, current + int(init_ticker_shape[0] * 0.8)))\n",
    "    test_indices.extend(np.arange(current + int(init_ticker_shape[0] * 0.8), current + init_ticker_shape[0]))\n",
    "    current += init_ticker_shape[0]\n",
    "train_folds=[]\n",
    "test_folds=[]\n",
    "for fold in np.arange(5):\n",
    "    train_folds.append(train_indices)\n",
    "    test_folds.append(test_indices)\n",
    "custom_cv = zip(train_folds, test_folds)\n",
    "\n",
    "params_grid_d = dict()\n",
    "fit_params_d = dict()\n",
    "grid_cv_d = dict()\n",
    "\n",
    "params_grid_d['dnn'] = {\n",
    "    #Обязательные параметры\n",
    "    #'agent__look_back': [max_look_back],\n",
    "    #'agent__verbose': [0],\n",
    "    \n",
    "    #Настраиваемые параметры\n",
    "    #'agent__batch_size':[1], #, 2, 3\n",
    "    #'agent__neurons': [8, 32], #, 128\n",
    "    #'agent__layers': [1, 2], #, 4\n",
    "    #'agent__init': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'],\n",
    "    \n",
    "    #'agent__activation': ['softplus', 'tanh', 'sigmoid', 'linear', 'softmax', 'hard_sigmoid', 'softsign', 'relu'],# \n",
    "    #РЕГУЛЯРИЗАТОРЫ\n",
    "    #'agent__b_W_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    #'agent__activity_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    #'agent__optimizer':['sgd', 'adam', 'nadam', 'rmsprop', 'adagrad', 'adamax', 'adadelta'], #\n",
    "    #'agent__lr': [0.001, 0.01, 0.1] #, 0.2, 0.3\n",
    "    #!!!!!!!!!W_constraint: b_constraint:\n",
    "    \n",
    "    #'agent__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],    \n",
    "    'agent__nb_epoch':  [10000], #, 20\n",
    "    \n",
    "    #Для проработки в будущем:\n",
    "    #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    #weight_constraint = [1, 2, 3, 4, 5]\n",
    "    }\n",
    "\n",
    "params_grid_d['lstm'] = {\n",
    "    'agent__neurons': [4, 8, 16, 32, 64], #, 128\n",
    "    'agent__layers': [2, 4], # 1, \n",
    "    #'agent__init': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'],\n",
    "    \n",
    "    'agent__activation': ['sigmoid', 'softmax', 'hard_sigmoid', 'relu'],# 'linear', 'softplus', 'softsign', 'tanh', \n",
    "    #РЕГУЛЯРИЗАТОРЫ\n",
    "    #'agent__b_W_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    #'agent__activity_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    'agent__optimizer':['sgd', 'adagrad', 'adadelta'], #'adam', 'nadam', 'rmsprop', 'adamax', \n",
    "    #'agent__lr': [0.001, 0.01, 0.1] #, 0.2, 0.3\n",
    "    #!!!!!!!!!W_constraint: b_constraint:\n",
    "    \n",
    "    #'agent__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],    \n",
    "    'agent__nb_epoch':  [10000], #, 20\n",
    "    \n",
    "    #Для проработки в будущем:\n",
    "    #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    #weight_constraint = [1, 2, 3, 4, 5]\n",
    "    }\n",
    "\n",
    "fit_params_d['lstm'] = {\n",
    "    #'agent__look_back': [max_look_back],\n",
    "    'agent__verbose': 0,\n",
    "    'agent__callbacks': solutions['lstm'].callbacks[0:2], # Не передаём автоматическое сохранение лучшего результата,.. пока\n",
    "}\n",
    "\n",
    "params_grid_d['lstm_mbb'] = {\n",
    "    #'agent__neurons': [4, 8, 16, 32, 64], #, 128\n",
    "    #'agent__layers': [2, 4], # 1, \n",
    "    #'agent__init': ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'],\n",
    "    \n",
    "    #'agent__activation': ['sigmoid', 'softmax', 'hard_sigmoid', 'relu'],# 'linear', 'softplus', 'softsign', 'tanh', \n",
    "    #РЕГУЛЯРИЗАТОРЫ\n",
    "    #'agent__b_W_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    #'agent__activity_regularizer': [None, 'l1', 'l2'], #, 'l1l2'\n",
    "    'agent__optimizer':['adagrad', 'adadelta'], #'sgd', 'adam', 'nadam', 'rmsprop', 'adamax', \n",
    "    #'agent__lr': [0.001, 0.01, 0.1] #, 0.2, 0.3\n",
    "    #!!!!!!!!!W_constraint: b_constraint:\n",
    "    \n",
    "    #'agent__dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],    \n",
    "    'agent__nb_epoch':  [6], #, 20\n",
    "    \n",
    "    #Для проработки в будущем:\n",
    "    #momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    #weight_constraint = [1, 2, 3, 4, 5]\n",
    "    }\n",
    "\n",
    "fit_params_d['lstm_mbb'] = {\n",
    "    #'agent__look_back': [max_look_back],\n",
    "    'agent__verbose': 0,\n",
    "    'agent__callbacks': solutions['lstm_mbb'].callbacks[0:2], # Не передаём автоматическое сохранение лучшего результата,.. пока\n",
    "}\n",
    "\n",
    "model = 'lstm' #'dnn'\n",
    "data_train_temp, target_train_temp = solutions[model].transform_dt(data_train, target_train)\n",
    "grid_cv_d[model] = GridSearchCV(solutions[model].pipeline, params_grid_d[model], fit_params=fit_params_d['lstm'] , cv=5, scoring='neg_mean_squared_error', n_jobs=-2, verbose=1) #custom_cv\n",
    "fit_grid(grid_cv_d[model], data_train_temp, target_train_temp, True, True)\n",
    "\n",
    "# Сохранение результатов GridSearchCV для анализа\n",
    "test_df = pd.DataFrame()\n",
    "for column in grid_cv_d[model].cv_results_.keys():\n",
    "    if (column[:13] != 'param_agent__') & (column[:6] != 'params') & (column[:5] != 'split'):\n",
    "        test_df[column] = grid_cv_d[model].cv_results_[column]\n",
    "for column in grid_cv_d[model].cv_results_.keys():\n",
    "    if (column[:13] == 'param_agent__'):\n",
    "        test_df[column[13:]] = grid_cv_d[model].cv_results_[column]\n",
    "\n",
    "writer = pd.ExcelWriter('..\\\\5. Evaluation\\\\GridSearchCV_' + model + '_' + dt.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\") + test_text + '.xlsx')\n",
    "test_df.to_excel(writer, 'Оценка', index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "params_grid['lstm'] = {\n",
    "    #Обязательные параметры\n",
    "    #'agent__look_back': [max_look_back],\n",
    "\n",
    "    #Настраиваемые параметры\n",
    "    'agent__nb_epoch':  [1, 20],\n",
    "    'agent__batch_size':[1, 2, 3],\n",
    "    #'agent__optimizer':['Adam', 'Nadam'],\n",
    "    }\n",
    "data_train_temp, target_train_temp = solutions['lstm'].transform_dt(data_train, target_train)\n",
    "grid_cv['lstm'] = GridSearchCV(solutions['lstm'].pipeline, params_grid['lstm'], cv=5, scoring='neg_mean_squared_error')\n",
    "#solutions['lstm'].pipeline = \n",
    "fit_grid(grid_cv['lstm'], data_train_temp, target_train_temp, True, True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for i in range (len(grid_cv['lstm'].cv_results_['params'])):\n",
    "    print (grid_cv['lstm'].cv_results_['mean_test_score'][i], grid_cv['lstm'].cv_results_['params'][i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "solutions['dnn'].pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print('\\n\\ndnn:')\n",
    "solutions['dnn'].fit(data_train, target_train, data_test, target_test) #5.9 e-04\n",
    "print('\\n\\nlstm:')\n",
    "solutions['lstm'].fit(data_train, target_train, data_test, target_test)\n",
    "print('\\n\\nlstm_mbb:')\n",
    "solutions['lstm_mbb'].fit(data_train, target_train, data_test, target_test)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(np.log(solutions['dnn'].callbacks[0].history['loss']))\n",
    "plt.plot(np.log(solutions['dnn'].callbacks[0].history['val_loss']))\n",
    "plt.plot(np.log(solutions['lstm'].callbacks[0].history['val_loss']))\n",
    "plt.plot(np.log(solutions['lstm_mbb'].callbacks[0].history['val_loss']))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['dnn', 'lstm', 'lstm_mbb'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>OPEN\n",
    "    Стандартное отклонение 0.02539131521854732   (2.5391e-2)\n",
    "    Дисперсия              0.0006447188885276327 (6.4471e-4)</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pfu_eval(val, model_type, data_type, measure):\n",
    "    '''Prepare for upload evaluation'''\n",
    "    val_df = pd.DataFrame([val],columns = ['Value'])\n",
    "    val_df['Модель'] = model_type\n",
    "    val_df['Тип данных'] = data_type\n",
    "    val_df['Measure'] = measure\n",
    "    val_df = val_df[['Модель', 'Тип данных', 'Measure', 'Value']]\n",
    "    return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(target, predict):\n",
    "    val_accuracy = 1-(target - predict).sum()/target.sum()\n",
    "    st_dev = sqrt(metrics.mean_squared_error(target, predict))\n",
    "    k_determ = metrics.r2_score(target, predict)\n",
    "    return val_accuracy, st_dev, k_determ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame(columns = ['Модель', 'Тип данных', 'Measure', 'Value'])\n",
    "\n",
    "train_lenght = index_train_d[tickers[0]].shape[0]\n",
    "test_lenght = index_test_d[tickers[0]].shape[0]\n",
    "\n",
    "for model_type in sorted(solutions):\n",
    "    print (model_type)\n",
    "    solution = solutions[model_type]\n",
    "    predict_train, target_train_temp = solution.predict(data_train, target_train)\n",
    "    predict_test, target_test_temp = solution.predict(data_test, target_test)\n",
    "    val_accuracy_train, st_dev_train, k_determ_train = calc_metrics(target_train_temp, predict_train)\n",
    "    val_accuracy_test, st_dev_test, k_determ_test = calc_metrics(target_test_temp, predict_test)\n",
    "    evaluation_df = evaluation_df.append(pd.concat([\n",
    "    #pfu_eval(val_accuracy_train, model_type, 'Train', 'Точность валовая'), \n",
    "    #pfu_eval(st_dev_train, model_type, 'Train', 'Ст. отклонение'), \n",
    "    pfu_eval(k_determ_train, model_type, 'Train', 'К. детерминации'), \n",
    "    #pfu_eval(val_accuracy_test, model_type, 'Test', 'Точность валовая'), \n",
    "    #pfu_eval(st_dev_test, model_type, 'Test', 'Ст. отклонение'),\n",
    "    pfu_eval(k_determ_test, model_type, 'Test', 'К. детерминации')\n",
    "            ]))\n",
    "\n",
    "    trainPredictPlot = np.empty_like(target_train.append (target_test).values)\n",
    "    testPredictPlot = np.empty_like((target_train.append (target_test)).values)\n",
    "    factPlot = np.empty_like((target_train.append (target_test)).values)\n",
    "    \n",
    "    \n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    \n",
    "    start_plot = 0\n",
    "    start_train = 0\n",
    "    start_test = 0\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        factPlot[:, :] = np.nan\n",
    "\n",
    "        trainPredictPlot[start_plot + max_look_back-1:start_plot + train_lenght, :] = predict_train[start_train:start_train+train_lenght]\n",
    "        factPlot[start_plot + max_look_back-1:start_plot + train_lenght, :] = target_train[start_train:start_train+train_lenght]\n",
    "\n",
    "        testPredictPlot[start_plot + train_lenght + max_look_back-1:start_plot + train_lenght + test_lenght, :] = predict_test[start_test:start_test+test_lenght]\n",
    "        factPlot[start_plot + train_lenght + max_look_back-1:start_plot + train_lenght + test_lenght, :] = target_test[start_test:start_test+test_lenght]\n",
    "        plt.plot(factPlot)\n",
    "        \n",
    "        start_train += train_lenght\n",
    "        start_test += test_lenght\n",
    "        start_plot += train_lenght + test_lenght\n",
    "    \n",
    "    #plt.plot((target_train.append (target_test)).values)\n",
    "    plt.plot(trainPredictPlot)\n",
    "    plt.plot(testPredictPlot)\n",
    "    plt.title(model_type)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_df#[evaluation_df['Тип данных'] == 'Test']\n",
    "\n",
    "writer = pd.ExcelWriter('..\\\\5. Evaluation\\\\FitEvaluation' + dt.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\") + '.xlsx')\n",
    "evaluation_df.to_excel(writer, 'Оценка', index = False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Version: GridSearchCV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# invert predictions\n",
    "predict_train = scaler.inverse_transform([predict_train])\n",
    "target_train = scaler.inverse_transform([target_train])\n",
    "predict_test = scaler.inverse_transform([predict_test])\n",
    "target_test = scaler.inverse_transform([target_test])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_train = predict_train.T\n",
    "predict_test = predict_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deployment"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "row = np.zeros((1,len(data_train.columns)))\n",
    "data_predict = pd.DataFrame(row, columns=data_train.columns)\n",
    "prediction_df = pd.DataFrame(columns = ['Value'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
